Our project is to implement a fully-replicated distributed file system that
supports simple view changes, multiple users and simultaneous editing of files.
To keep the project manageable, a number of simplifying assumptions have been
put in place that limit the use cases under which the system will be used. Our
primary use case is similar to that of Dropbox-- sharing will primarily be for a
single user, and while files will need to be editable globally, they will not
likely be edited in multiple locations in rapid succession.

Because we are implementing a file system, we must support causal orderings of
operations from each individual host to the others. However, since we expect
that there will either only be a single user on a single host at a time, or that
multiple hosts will be editing disjoint sets of files, the system will not
support a total ordering of file system changes between hosts. This use case
also allows us to support a lazier ‘eventual’ consistency model in which all
hosts they will eventually see all of the changes to the file system.

To help ensure that hosts edit disjoint sets of files, file trees will be
synchronously locked with multiple-reader-single-writer locks when files are
opened for editing. This will permit users on other hosts to see changes as they
happen, but not interfere with those changes (as doing so would require
acquiring an exclusive lock).

Operations originating on host “A” will be queued locally and also broadcast to
be queued on each of the other hosts in a queue specifically for A’s requests.
The system will handle the requests in order. Requests will ‘succeed’ once a
majority of hosts have acknowledged that the instruction was queued (for
asynchronous requests) or the majority of hosts have processed the queue up
until the new operation (for synchronous requests)..

The system is designed to be fairly simple, and as a result, we’ve chosen to
handle view changes in a very simple way: nodes are ‘lost’ when they can’t
contact the majority of hosts. We assume that packet transmission is fairly
reliable, and that the network won’t have partial or frequent outages. If nodes
detect that they’re unable to connect to the majority of hosts, it is up to them
to ensure that they stop accepting new file system modifications, and attempt to
reconnect (or simply restart). Other nodes can also inform a node that that node
‘died’ (because it previously wasn’t responding) at which point, it will have to
restart.

When nodes rejoin the system, they request a global lock on the file system. At
this point, nodes will temporarily not permit writes (either by blocking on
write or by having writes fail), they will sync to their disks, and then the
joining node will synchronize with another node using rsync. Upon completion (or
after a timeout, if the system fails), the lock is released and the system
resumes. Because of the choice to use rsync, nodes must be able to reach each
other via an rsync-compatible method (like ssh) as well as via our own RPC
calls.

For simplicity, we are using FUSE to implement the local filesystem, and Thrift
for IPC. The use of Thrift effectively guarantees that this system will not be
likely to achieve very good performance. Given our consistency model, this is an
acceptable tradeoff. Moreover, thrift uses stubs that would be fairly easy to
replace with a higher performance IPC method down the line if we were interested
in extending the system. To further keep the system and logic simple, the system
will not support linking (either hard or symbolic), FIFOs, nor extended
attributes. Depending on how the system progresses, we may opt to remove
non-essential file mode permissions as well. Given the tools used and our
intended testing plan, we will only guarantee that our software artifact works
on installations of Ubuntu  12.04 with FUSE and Thrift installed.

Currently, the system is at a fairly young stage. We have working ‘local’ file
systems using FUSE. We can also communicate between instances of FUSE via
Thrift, but operations are not executed remotely. We encountered more trouble
than we care to admit getting FUSE and Thrift to coexist, and so that
represented a significant time sink.

Because of our choice of IPC, we will not be measuring success with substantial
benchmarking. We plan on developing a small test suite that will test
functionality of the system across 3-5 nodes. This will consist of nodes dying
and rejoining, and moving files into and out of the file system and checking
accuracy of those files (likely with hashes).

Given our host needs, we plan on just using the nodes that have already been set
up for class use (i.e. the systems used for labs 1-3). Given that these nodes
already have Thrift and FUSE installed, we don’t need anything additional.

